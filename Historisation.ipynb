{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, current_timestamp\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"CustomerHistorization\").getOrCreate()\n",
    "\n",
    "# Simulate an initial load of customer data\n",
    "initial_data = [(1, 'Alice', 'alice@example.com'),\n",
    "                (2, 'Bob', 'bob@example.com'),\n",
    "                (3, 'Charlie', 'charlie@example.com')]\n",
    "\n",
    "columns = [\"CustomerID\", \"Name\", \"Email\"]\n",
    "\n",
    "df = spark.createDataFrame(initial_data, columns)\n",
    "\n",
    "\n",
    "# Simulate changes in the data (updates)\n",
    "# TODO: Inner join between df and incoming_df\n",
    "changes_data = [\n",
    "    (1, 'Alice', 'alice.new@example.com'),\n",
    "    (3, 'Chandler','charlie@example.com'),\n",
    "]\n",
    "\n",
    "changes_df = spark.createDataFrame(changes_data, columns)\n",
    "\n",
    "# Load incremental data (e.g., new customers)\n",
    "# TODO: remaining rows from incoming df\n",
    "incremental_data = [(4, 'David', 'david@example.com'),\n",
    "                    (5, 'Eva', 'eva@example.com')]\n",
    "\n",
    "incremental_df = spark.createDataFrame(incremental_data, columns)\n",
    "\n",
    "\n",
    "# Simulate deletions\n",
    "deletions = [\n",
    "    2,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the update df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------------------+------------+\n",
      "|CustomerID|   Name|              Email|  ChangeType|\n",
      "+----------+-------+-------------------+------------+\n",
      "|         2|    Bob|    bob@example.com|    Deletion|\n",
      "|         1|  Alice|  alice@example.com|Modification|\n",
      "|         3|Charlie|charlie@example.com|Modification|\n",
      "|         4|   null|               null|    Addition|\n",
      "|         5|   null|               null|    Addition|\n",
      "+----------+-------+-------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame for the update record\n",
    "\n",
    "# Record deletion operations\n",
    "update = (\n",
    "    df\n",
    "    .filter(F.col(\"CustomerID\").isin(deletions))\n",
    "    .withColumn(\"ChangeType\", lit(\"Deletion\"))\n",
    ")\n",
    "\n",
    "# Record modification operations\n",
    "update = (\n",
    "    update\n",
    "    .unionByName(\n",
    "        df\n",
    "        .join(changes_df, [\"CustomerID\"], \"leftsemi\")\n",
    "        .withColumn(\"ChangeType\", lit(\"Modification\"))   \n",
    "    )\n",
    "    \n",
    ")\n",
    "\n",
    "# Record addition operations (new row)\n",
    "# All columns are not needed just the keys\n",
    "copy_incremental_df = incremental_df\n",
    "\n",
    "null_cols = [x for x in columns if x != \"CustomerID\"]\n",
    "for col in null_cols:\n",
    "    copy_incremental_df = copy_incremental_df.withColumn(col, F.lit(None))\n",
    "\n",
    "update = (\n",
    "    update\n",
    "    .unionByName(\n",
    "        copy_incremental_df\n",
    "        .withColumn(\"ChangeType\", lit(\"Addition\"))\n",
    "    )\n",
    "    \n",
    ")\n",
    "\n",
    "update.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform the operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reverse the step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pyspark)",
   "language": "python",
   "name": "pyspark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
